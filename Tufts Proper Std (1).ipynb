{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f80346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def process_excel_sheet(folder_path, group_name):\n",
    "    def process_data(data):\n",
    "    # Identify datetime columns\n",
    "        date_columns = data.select_dtypes(include=[np.datetime64]).columns\n",
    "    \n",
    "    # Convert datetime columns to string\n",
    "        data[date_columns] = data[date_columns].apply(lambda x: x.astype(str) if x.name in date_columns else x)\n",
    "    \n",
    "        # Create a DataFrame for the sheet name with the same number of columns as data\n",
    "        sheet_name_df = pd.DataFrame([sheet_name] * len(data.columns)).T\n",
    "        sheet_name_df.columns = data.columns\n",
    "        \n",
    "        # Concatenate sheet name DataFrame with the original data\n",
    "        data = pd.concat([sheet_name_df, data], ignore_index=True)\n",
    "        \n",
    "        # Iterate through the DataFrame rows\n",
    "        for index, row in data.iterrows():\n",
    "            for col in range(len(row)):\n",
    "                # If the cell contains \"PRIMARY/SOLD PLAN\"\n",
    "                if row[col] == \"PRIMARY/SOLD PLAN\":\n",
    "                    # Fill all cells to the left with the value \"PRIMARY/SOLD PLAN\"\n",
    "                    for i in range(col - 1, -1, -1):\n",
    "                        data.iloc[index, i] = row[col]\n",
    "\n",
    "        # Extract unique options from the DataFrame\n",
    "        unique_options = set()\n",
    "        for index, row in data.iterrows():\n",
    "            for cell in row:\n",
    "                if isinstance(cell, str):  # Check if the cell is a string\n",
    "                    options = re.findall(r'OPTION \\d+', cell)  # Extract options using regex\n",
    "                    unique_options.update(options)\n",
    "\n",
    "        # Iterate through the DataFrame rows\n",
    "        for index, row in data.iterrows():\n",
    "            for col in range(len(row)):\n",
    "                # If the cell contains any of the unique options\n",
    "                for option in unique_options:\n",
    "                    if isinstance(row[col], str) and option in row[col]:\n",
    "                        # Fill all cells to the left with the corresponding value\n",
    "                        for i in range(col - 1, -1, -1):\n",
    "                            data.iloc[index, i] = row[col]\n",
    "                        break  # Break out of the loop once the option is found\n",
    "\n",
    "        # Keeping only columns Unnamed: 4 to Unnamed: 7\n",
    "        df = data.iloc[:, 4:8]\n",
    "\n",
    "        # Define a function to shift non-NaN values to the left\n",
    "        def shift_non_nan(row):\n",
    "            shifted_row = [np.nan] * len(row)\n",
    "            last_non_nan_index = None\n",
    "            for i, value in enumerate(row):\n",
    "                if pd.notnull(value):\n",
    "                    if last_non_nan_index is None:\n",
    "                        shifted_row[0] = value\n",
    "                        last_non_nan_index = 0\n",
    "                    else:\n",
    "                        last_non_nan_index += 1\n",
    "                        shifted_row[last_non_nan_index] = value\n",
    "            return pd.Series(shifted_row)\n",
    "\n",
    "        # Apply the function to each row of the dataframe\n",
    "        df = df.apply(shift_non_nan, axis=1)\n",
    "\n",
    "        # Find the rows where 'Benefit Plan' is mentioned\n",
    "        benefit_plan_rows = df[df.apply(lambda row: 'Benefit Plan' in row.values, axis=1)]\n",
    "\n",
    "        # Merge cells to the right of 'Benefit Plan' rows\n",
    "        for index, row in benefit_plan_rows.iterrows():\n",
    "            # Find the index of the column where 'Benefit Plan' is located\n",
    "            benefit_plan_index = np.where(row == 'Benefit Plan')[0][0]\n",
    "            # Find non-NaN values to merge\n",
    "            non_nan_values = row.iloc[benefit_plan_index + 1:].dropna().tolist()\n",
    "            # Merge non-NaN values into a single cell\n",
    "            merged_value = ' '.join(str(cell) for cell in non_nan_values)\n",
    "            # Assign the merged value to the cell next to 'Benefit Plan'\n",
    "            df.at[index, benefit_plan_index + 1] = merged_value\n",
    "\n",
    "        # Assuming your DataFrame is named df\n",
    "        df = df.drop(columns=[2, 3])\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Function to split cells containing commas into separate rows for applicable columns\n",
    "        def split_rows(df):\n",
    "            new_rows = []\n",
    "            for index, row in df.iterrows():\n",
    "                split_row = False  # Flag to check if splitting occurred for this row\n",
    "                for column_name, cell_value in row.items():\n",
    "                    if ',' in str(cell_value):\n",
    "                        split_row = True  # Set flag to True if splitting occurred for any column\n",
    "                        details = str(cell_value).split(', ')\n",
    "                        for detail in details:\n",
    "                            new_row = row.copy()  # Copy the original row\n",
    "                            new_row[column_name] = detail.strip()  # Update the cell value\n",
    "                            new_rows.append(new_row)  # Append the new row to the list\n",
    "                        break  # Break out of the loop once splitting occurs for any column\n",
    "                if not split_row:\n",
    "                    new_rows.append(row)  # Append the original row if no splitting occurred\n",
    "            new_df = pd.DataFrame(new_rows)  # Creating a new DataFrame with split rows\n",
    "            return new_df\n",
    "\n",
    "        # Call the function with the DataFrame\n",
    "        df = split_rows(df)\n",
    "\n",
    "        # Assign unique indices\n",
    "        df.index = range(len(df))\n",
    "\n",
    "        # Words to check for deletion\n",
    "        words_to_delete = ['OP', 'IP', 'MD', 'X-Ray', 'Lab', 'Riders', 'Drug Benefit', 'Rating Group']\n",
    "\n",
    "        # Function to filter out rows containing specified words\n",
    "        def filter_rows(df, words_to_delete):\n",
    "            indices_to_drop = []  # List to store indices of rows to drop\n",
    "            # Iterate through each row\n",
    "            for index, row in df.iterrows():\n",
    "                # Flag to check if any word to delete is found\n",
    "                delete_row = False\n",
    "                # Iterate through each cell value in the row\n",
    "                for cell_value in row.values:\n",
    "                    # Check if any word to delete is present in the cell value\n",
    "                    for word in words_to_delete:\n",
    "                        if re.search(r'\\b' + word + r'\\b', str(cell_value)):\n",
    "                            # If any word is found, set delete_row flag to True\n",
    "                            delete_row = True\n",
    "                            break  # No need to continue checking for words in this row\n",
    "                    if delete_row:\n",
    "                        break  # No need to check further if any word is found in this row\n",
    "                if delete_row:\n",
    "                    indices_to_drop.append(index)  # Store the index of the row to drop\n",
    "            # Drop the rows outside of the loop to avoid modifying DataFrame while iterating\n",
    "            df_filtered = df.drop(indices_to_drop)\n",
    "            return df_filtered\n",
    "\n",
    "        # Call the function with the DataFrame\n",
    "        df = filter_rows(df, words_to_delete)\n",
    "\n",
    "        # Add a space before each $ sign in the DataFrame\n",
    "        df = df.applymap(lambda x: str(x).replace('$', ' $') if isinstance(x, str) else x)\n",
    "\n",
    "        # Extracting the words \"PCP\", \"SPC\", \"UC\", \"ER\", \"Ded\", and \"Coins\" into a separate column\n",
    "        df.insert(loc=1, column='Type', value=df[1].str.extract(r'(PCP|SPC|UC|ER|Ded|Coins|OOP)', expand=False))\n",
    "\n",
    "        # Fill NaN values in the \"Type\" column with a default value\n",
    "        df['Type'].fillna('', inplace=True)\n",
    "\n",
    "        # Remove the words \"PCP\", \"SPC\", \"UC\", \"ER\", \"Ded\", and \"Coins\" from the actual column\n",
    "        df[1] = df[1].str.replace(r'(PCP|SPC|UC|ER|Ded|Coins|OOP)', '', regex=True)\n",
    "\n",
    "        # Merge the first and second columns\n",
    "        df.insert(loc=0, column='Merged', value=df[0].astype(str) + \" \" + df['Type'].astype(str))\n",
    "\n",
    "        # Drop the 'Type' and the original first column\n",
    "        df.drop(columns=[0, 'Type'], inplace=True)\n",
    "\n",
    "        # Set column names to empty strings\n",
    "        df.columns = [''] * len(df.columns)\n",
    "\n",
    "        # Reset index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Fill the first NaN value in the first column with 'Group Name'\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].fillna('Group Name', limit=1)\n",
    "\n",
    "        # Fill the second NaN value in the first column with 'MNS ID'\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].fillna('MNS ID', limit=1)\n",
    "\n",
    "        # Set column names to empty strings\n",
    "        df.columns = [''] * len(df.columns)\n",
    "\n",
    "        # Reset index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Replace words in the DataFrame using the mapping dictionary\n",
    "        word_mapping = {\n",
    "            'In Network Copays PCP': 'INN PCP COPAY',\n",
    "            'In Network Copays SPC': 'INN SPEC COPAY',\n",
    "            'In Network Copays UC': 'INN URGENT CARE COPAY',\n",
    "            'In Network Copays ER': 'INN ER COPAY',\n",
    "            'In Net Ded/Coins/OOP Ded': 'INN DEDUCTIBLE',\n",
    "            'In Net Ded/Coins/OOP Coins': 'INN COINSURANCE',\n",
    "            'In Net Ded/Coins/OOP OOP': 'INN OOP Max.',\n",
    "            'Out of Network Ded': 'OON DEDUCTIBLE',\n",
    "            'Out of Network Coins': 'OON COINSURANCE',\n",
    "            'Out of Network OOP': 'OON OOP Max.'\n",
    "        }\n",
    "\n",
    "        # Replace words in the DataFrame using the mapping dictionary\n",
    "        df.replace(word_mapping, inplace=True)\n",
    "\n",
    "        # Create a new DataFrame to store the result\n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "        # Iterate over each row in the original DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            # Check if the row in the second column contains '/'\n",
    "            if '/' in row[1]:  # Assuming the second column is the one you want to check\n",
    "                # Split the row into two rows if '/' has '$' before or after it\n",
    "                parts = row[1].split('/')\n",
    "                if ('$' in parts[0]) or ('$' in parts[1]):\n",
    "                    # Add 'IND' prefix to the first split row and 'FAM' to the second split row if necessary\n",
    "                    new_df.loc[len(new_df)] = ['IND ' + row[0], parts[0]]\n",
    "                    new_df.loc[len(new_df)] = ['FAM ' + row[0], parts[1]]\n",
    "                else:\n",
    "                    new_df.loc[len(new_df)] = [row[0], row[1]]\n",
    "            else:\n",
    "                # If no '/', append the row as it is to the new DataFrame\n",
    "                new_df.loc[len(new_df)] = [row[0], row[1]]\n",
    "\n",
    "        # Return the processed DataFrame\n",
    "        return new_df\n",
    "\n",
    "    \n",
    "    result_dfs = []  # List to store processed DataFrames\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Skip files starting with '~$'\n",
    "        if not file_name.startswith('~$'):\n",
    "            if file_name.endswith('.xlsx'):\n",
    "                # Read the Excel file\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                try:\n",
    "                    # Read all sheets from the Excel file\n",
    "                    sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "                    # Iterate over each sheet\n",
    "                    for sheet_name, data in sheets.items():\n",
    "                        # Check if the sheet_name matches the specified group_name\n",
    "                        if group_name in sheet_name:\n",
    "                            # Process the data for the current sheet\n",
    "                            processed_data = process_data(data)\n",
    "                            # Replace the first row, first column with \"Name\"\n",
    "                            processed_data.iloc[0, 0] = \"Name\"\n",
    "                            \n",
    "                            # Create a new DataFrame containing rows indexed 0 and 1\n",
    "                            header_df = processed_data.iloc[:2]\n",
    "                            \n",
    "                            # Split the DataFrame based on rows starting with \"OPTION\"\n",
    "                            split_dfs = []\n",
    "                            current_df = None\n",
    "                            for index, row in processed_data.iterrows():\n",
    "                                if row[0].startswith('OPTION'):\n",
    "                                    if current_df is not None:\n",
    "                                        # Concatenate current_df with header_df before appending\n",
    "                                        split_dfs.append(pd.concat([header_df, current_df], ignore_index=True))\n",
    "                                    current_df = pd.DataFrame(columns=processed_data.columns)\n",
    "                                if current_df is not None:\n",
    "                                    current_df = pd.concat([current_df, row.to_frame().T], ignore_index=True)\n",
    "                            if current_df is not None:\n",
    "                                # Concatenate current_df with header_df before appending\n",
    "                                split_dfs.append(pd.concat([header_df, current_df], ignore_index=True))\n",
    "                            \n",
    "                            result_dfs.extend(split_dfs)  # Add split DataFrames to the result list\n",
    "                            \n",
    "                except PermissionError as e:\n",
    "                    print(f\"Permission error occurred while accessing {file_name}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while processing {file_name}: {e}\")\n",
    "    \n",
    "    return result_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ca1631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Name</td>\n",
       "      <td>Greater Noida Mental MNS0000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EFFECTIVE DATE</td>\n",
       "      <td>1/1/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OPTION 2</td>\n",
       "      <td>OPTION 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRIMARY/SOLD PLAN</td>\n",
       "      <td>PRIMARY/SOLD PLAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Benefit Plan</td>\n",
       "      <td>CPIJ Mod (Granite Advantage EPO OA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>IND INN PCP COPAY</td>\n",
       "      <td>$20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FAM INN PCP COPAY</td>\n",
       "      <td>$20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>IND INN SPEC COPAY</td>\n",
       "      <td>$35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FAM INN SPEC COPAY</td>\n",
       "      <td>$35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INN URGENT CARE COPAY</td>\n",
       "      <td>$20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>INN ER COPAY</td>\n",
       "      <td>$330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>IND INN DEDUCTIBLE</td>\n",
       "      <td>$6,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FAM INN DEDUCTIBLE</td>\n",
       "      <td>$12,000 (Emb)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>INN COINSURANCE</td>\n",
       "      <td>90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>IND INN OOP Max.</td>\n",
       "      <td>$7,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>FAM INN OOP Max.</td>\n",
       "      <td>$12,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>OON DEDUCTIBLE</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>OON COINSURANCE</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OON OOP Max.</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              \n",
       "0                    Name      Greater Noida Mental MNS0000192\n",
       "1         EFFECTIVE DATE                              1/1/2024\n",
       "2               OPTION 2                              OPTION 2\n",
       "3      PRIMARY/SOLD PLAN                     PRIMARY/SOLD PLAN\n",
       "4           Benefit Plan   CPIJ Mod (Granite Advantage EPO OA)\n",
       "5       IND INN PCP COPAY                                  $20\n",
       "6       FAM INN PCP COPAY                                  $20\n",
       "7      IND INN SPEC COPAY                                  $35\n",
       "8      FAM INN SPEC COPAY                                  $35\n",
       "9   INN URGENT CARE COPAY                                  $20\n",
       "10           INN ER COPAY                                 $330\n",
       "11     IND INN DEDUCTIBLE                               $6,000\n",
       "12     FAM INN DEDUCTIBLE                        $12,000 (Emb)\n",
       "13        INN COINSURANCE                                  90%\n",
       "14       IND INN OOP Max.                               $7,000\n",
       "15       FAM INN OOP Max.                              $12,000\n",
       "16         OON DEDUCTIBLE                                  N/A\n",
       "17        OON COINSURANCE                                  N/A\n",
       "18           OON OOP Max.                                  N/A"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the folder containing the Excel files\n",
    "folder_path = r\"C:\\Users\\shres\\Downloads\\Extracted_Tables_Tuftsss\"\n",
    "\n",
    "# Specify the group name you want to process\n",
    "group_name = \"Greater Noida Mental\"  # Change this to the desired group name\n",
    "\n",
    "# Call the function to process the specified group name in the folder\n",
    "split_dfs = process_excel_sheet(folder_path, group_name)\n",
    "split_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c04a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def process_excel_sheet(folder_path, group_name):\n",
    "    def process_data(data):\n",
    "    # Identify datetime columns\n",
    "        date_columns = data.select_dtypes(include=[np.datetime64]).columns\n",
    "    \n",
    "    # Convert datetime columns to string\n",
    "        data[date_columns] = data[date_columns].apply(lambda x: x.astype(str) if x.name in date_columns else x)\n",
    "        \n",
    "        # Extracting sheet name without \"MNS\"\n",
    "        filtered_sheet_name = re.sub(r'\\bMNS.*', '', sheet_name).strip()\n",
    "\n",
    "# Create a DataFrame for the filtered sheet name with the same number of columns as data\n",
    "        filtered_sheet_name_df = pd.DataFrame([filtered_sheet_name] * len(data.columns)).T\n",
    "        filtered_sheet_name_df.columns = data.columns\n",
    "\n",
    "# Concatenate filtered sheet name DataFrame with the original data\n",
    "        data = pd.concat([filtered_sheet_name_df, data], ignore_index=True)\n",
    "        \n",
    "        # Iterate through the DataFrame rows\n",
    "        for index, row in data.iterrows():\n",
    "            for col in range(len(row)):\n",
    "                # If the cell contains \"PRIMARY/SOLD PLAN\"\n",
    "                if row[col] == \"PRIMARY/SOLD PLAN\":\n",
    "                    # Fill all cells to the left with the value \"PRIMARY/SOLD PLAN\"\n",
    "                    for i in range(col - 1, -1, -1):\n",
    "                        data.iloc[index, i] = row[col]\n",
    "\n",
    "        # Extract unique options from the DataFrame\n",
    "        unique_options = set()\n",
    "        for index, row in data.iterrows():\n",
    "            for cell in row:\n",
    "                if isinstance(cell, str):  # Check if the cell is a string\n",
    "                    options = re.findall(r'OPTION \\d+', cell)  # Extract options using regex\n",
    "                    unique_options.update(options)\n",
    "\n",
    "        # Iterate through the DataFrame rows\n",
    "        for index, row in data.iterrows():\n",
    "            for col in range(len(row)):\n",
    "                # If the cell contains any of the unique options\n",
    "                for option in unique_options:\n",
    "                    if isinstance(row[col], str) and option in row[col]:\n",
    "                        # Fill all cells to the left with the corresponding value\n",
    "                        for i in range(col - 1, -1, -1):\n",
    "                            data.iloc[index, i] = row[col]\n",
    "                        break  # Break out of the loop once the option is found\n",
    "\n",
    "        # Keeping only columns Unnamed: 4 to Unnamed: 7\n",
    "        df = data.iloc[:, 4:8]\n",
    "\n",
    "        # Define a function to shift non-NaN values to the left\n",
    "        def shift_non_nan(row):\n",
    "            shifted_row = [np.nan] * len(row)\n",
    "            last_non_nan_index = None\n",
    "            for i, value in enumerate(row):\n",
    "                if pd.notnull(value):\n",
    "                    if last_non_nan_index is None:\n",
    "                        shifted_row[0] = value\n",
    "                        last_non_nan_index = 0\n",
    "                    else:\n",
    "                        last_non_nan_index += 1\n",
    "                        shifted_row[last_non_nan_index] = value\n",
    "            return pd.Series(shifted_row)\n",
    "\n",
    "        # Apply the function to each row of the dataframe\n",
    "        df = df.apply(shift_non_nan, axis=1)\n",
    "\n",
    "        # Find the rows where 'Benefit Plan' is mentioned\n",
    "        benefit_plan_rows = df[df.apply(lambda row: 'Benefit Plan' in row.values, axis=1)]\n",
    "\n",
    "        # Merge cells to the right of 'Benefit Plan' rows\n",
    "        for index, row in benefit_plan_rows.iterrows():\n",
    "            # Find the index of the column where 'Benefit Plan' is located\n",
    "            benefit_plan_index = np.where(row == 'Benefit Plan')[0][0]\n",
    "            # Find non-NaN values to merge\n",
    "            non_nan_values = row.iloc[benefit_plan_index + 1:].dropna().tolist()\n",
    "            # Merge non-NaN values into a single cell\n",
    "            merged_value = ' '.join(str(cell) for cell in non_nan_values)\n",
    "            # Assign the merged value to the cell next to 'Benefit Plan'\n",
    "            df.at[index, benefit_plan_index + 1] = merged_value\n",
    "\n",
    "        # Assuming your DataFrame is named df\n",
    "        df = df.drop(columns=[2, 3])\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Function to split cells containing commas into separate rows for applicable columns\n",
    "        def split_rows(df):\n",
    "            new_rows = []\n",
    "            for index, row in df.iterrows():\n",
    "                split_row = False  # Flag to check if splitting occurred for this row\n",
    "                for column_name, cell_value in row.items():\n",
    "                    if ',' in str(cell_value):\n",
    "                        split_row = True  # Set flag to True if splitting occurred for any column\n",
    "                        details = str(cell_value).split(', ')\n",
    "                        for detail in details:\n",
    "                            new_row = row.copy()  # Copy the original row\n",
    "                            new_row[column_name] = detail.strip()  # Update the cell value\n",
    "                            new_rows.append(new_row)  # Append the new row to the list\n",
    "                        break  # Break out of the loop once splitting occurs for any column\n",
    "                if not split_row:\n",
    "                    new_rows.append(row)  # Append the original row if no splitting occurred\n",
    "            new_df = pd.DataFrame(new_rows)  # Creating a new DataFrame with split rows\n",
    "            return new_df\n",
    "\n",
    "        # Call the function with the DataFrame\n",
    "        df = split_rows(df)\n",
    "\n",
    "        # Assign unique indices\n",
    "        df.index = range(len(df))\n",
    "\n",
    "        # Words to check for deletion\n",
    "        words_to_delete = ['OP', 'IP', 'MD', 'X-Ray', 'Lab', 'Riders', 'Drug Benefit', 'Rating Group']\n",
    "\n",
    "        # Function to filter out rows containing specified words\n",
    "        def filter_rows(df, words_to_delete):\n",
    "            indices_to_drop = []  # List to store indices of rows to drop\n",
    "            # Iterate through each row\n",
    "            for index, row in df.iterrows():\n",
    "                # Flag to check if any word to delete is found\n",
    "                delete_row = False\n",
    "                # Iterate through each cell value in the row\n",
    "                for cell_value in row.values:\n",
    "                    # Check if any word to delete is present in the cell value\n",
    "                    for word in words_to_delete:\n",
    "                        if re.search(r'\\b' + word + r'\\b', str(cell_value)):\n",
    "                            # If any word is found, set delete_row flag to True\n",
    "                            delete_row = True\n",
    "                            break  # No need to continue checking for words in this row\n",
    "                    if delete_row:\n",
    "                        break  # No need to check further if any word is found in this row\n",
    "                if delete_row:\n",
    "                    indices_to_drop.append(index)  # Store the index of the row to drop\n",
    "            # Drop the rows outside of the loop to avoid modifying DataFrame while iterating\n",
    "            df_filtered = df.drop(indices_to_drop)\n",
    "            return df_filtered\n",
    "\n",
    "        # Call the function with the DataFrame\n",
    "        df = filter_rows(df, words_to_delete)\n",
    "\n",
    "        # Add a space before each $ sign in the DataFrame\n",
    "        df = df.applymap(lambda x: str(x).replace('$', ' $') if isinstance(x, str) else x)\n",
    "\n",
    "        # Extracting the words \"PCP\", \"SPC\", \"UC\", \"ER\", \"Ded\", and \"Coins\" into a separate column\n",
    "        df.insert(loc=1, column='Type', value=df[1].str.extract(r'(PCP|SPC|UC|ER|Ded|Coins|OOP)', expand=False))\n",
    "\n",
    "        # Fill NaN values in the \"Type\" column with a default value\n",
    "        df['Type'].fillna('', inplace=True)\n",
    "\n",
    "        # Remove the words \"PCP\", \"SPC\", \"UC\", \"ER\", \"Ded\", and \"Coins\" from the actual column\n",
    "        df[1] = df[1].str.replace(r'(PCP|SPC|UC|ER|Ded|Coins|OOP)', '', regex=True)\n",
    "\n",
    "        # Merge the first and second columns\n",
    "        df.insert(loc=0, column='Merged', value=df[0].astype(str) + \" \" + df['Type'].astype(str))\n",
    "\n",
    "        # Drop the 'Type' and the original first column\n",
    "        df.drop(columns=[0, 'Type'], inplace=True)\n",
    "\n",
    "        # Set column names to empty strings\n",
    "        df.columns = [''] * len(df.columns)\n",
    "\n",
    "        # Reset index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Fill the first NaN value in the first column with 'Group Name'\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].fillna('Group Name', limit=1)\n",
    "\n",
    "        # Fill the second NaN value in the first column with 'MNS ID'\n",
    "        df.iloc[:, 0] = df.iloc[:, 0].fillna('MNS ID', limit=1)\n",
    "\n",
    "        # Set column names to empty strings\n",
    "        df.columns = [''] * len(df.columns)\n",
    "\n",
    "        # Reset index\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Replace words in the DataFrame using the mapping dictionary\n",
    "        word_mapping = {\n",
    "            'In Network Copays PCP': 'INN PCP COPAY',\n",
    "            'In Network Copays SPC': 'INN SPEC COPAY',\n",
    "            'In Network Copays UC': 'INN URGENT CARE COPAY',\n",
    "            'In Network Copays ER': 'INN ER COPAY',\n",
    "            'In Net Ded/Coins/OOP Ded': 'INN DEDUCTIBLE',\n",
    "            'In Net Ded/Coins/OOP Coins': 'INN COINSURANCE',\n",
    "            'In Net Ded/Coins/OOP OOP': 'INN OOP Max.',\n",
    "            'Out of Network Ded': 'OON DEDUCTIBLE',\n",
    "            'Out of Network Coins': 'OON COINSURANCE',\n",
    "            'Out of Network OOP': 'OON OOP Max.'\n",
    "        }\n",
    "\n",
    "        # Replace words in the DataFrame using the mapping dictionary\n",
    "        df.replace(word_mapping, inplace=True)\n",
    "\n",
    "        # Create a new DataFrame to store the result\n",
    "        new_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "        # Iterate over each row in the original DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            # Check if the row in the second column contains '/'\n",
    "            if '/' in row[1]:  # Assuming the second column is the one you want to check\n",
    "                # Split the row into two rows if '/' has '$' before or after it\n",
    "                parts = row[1].split('/')\n",
    "                if ('$' in parts[0]) or ('$' in parts[1]):\n",
    "                    # Add 'IND' prefix to the first split row and 'FAM' to the second split row if necessary\n",
    "                    new_df.loc[len(new_df)] = ['IND ' + row[0], parts[0]]\n",
    "                    new_df.loc[len(new_df)] = ['FAM ' + row[0], parts[1]]\n",
    "                else:\n",
    "                    new_df.loc[len(new_df)] = [row[0], row[1]]\n",
    "            else:\n",
    "                # If no '/', append the row as it is to the new DataFrame\n",
    "                new_df.loc[len(new_df)] = [row[0], row[1]]\n",
    "\n",
    "        # Return the processed DataFrame\n",
    "        return new_df\n",
    "\n",
    "    \n",
    "    result_dfs = []  # List to store processed DataFrames\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Skip files starting with '~$'\n",
    "        if not file_name.startswith('~$'):\n",
    "            if file_name.endswith('.xlsx'):\n",
    "                # Read the Excel file\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                try:\n",
    "                    # Read all sheets from the Excel file\n",
    "                    sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "                    # Iterate over each sheet\n",
    "                    for sheet_name, data in sheets.items():\n",
    "                        # Check if the sheet_name matches the specified group_name\n",
    "                        if group_name in sheet_name:\n",
    "                            # Process the data for the current sheet\n",
    "                            processed_data = process_data(data)\n",
    "                            # Replace the first row, first column with \"Name\"\n",
    "                            processed_data.iloc[0, 0] = \"Name\"\n",
    "                            \n",
    "                            # Create a new DataFrame containing rows indexed 0 and 1\n",
    "                            header_df = processed_data.iloc[:2]\n",
    "                            \n",
    "                            # Split the DataFrame based on rows starting with \"OPTION\"\n",
    "                            split_dfs = []\n",
    "                            current_df = None\n",
    "                            for index, row in processed_data.iterrows():\n",
    "                                if row[0].startswith('OPTION'):\n",
    "                                    if current_df is not None:\n",
    "                                        # Concatenate current_df with header_df before appending\n",
    "                                        split_dfs.append(pd.concat([header_df, current_df], ignore_index=True))\n",
    "                                    current_df = pd.DataFrame(columns=processed_data.columns)\n",
    "                                if current_df is not None:\n",
    "                                    current_df = pd.concat([current_df, row.to_frame().T], ignore_index=True)\n",
    "                            if current_df is not None:\n",
    "                                # Concatenate current_df with header_df before appending\n",
    "                                split_dfs.append(pd.concat([header_df, current_df], ignore_index=True))\n",
    "                            \n",
    "                            result_dfs.extend(split_dfs)  # Add split DataFrames to the result list\n",
    "                            \n",
    "                except PermissionError as e:\n",
    "                    print(f\"Permission error occurred while accessing {file_name}: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while processing {file_name}: {e}\")\n",
    "    \n",
    "    return result_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e3db9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                                              \n",
       " 0                    Name                 Greater Noida Mental\n",
       " 1         EFFECTIVE DATE                              1/1/2024\n",
       " 2               OPTION 1                              OPTION 1\n",
       " 3      PRIMARY/SOLD PLAN                     PRIMARY/SOLD PLAN\n",
       " 4           Benefit Plan   CPIJ Mod (Granite Advantage EPO OA)\n",
       " 5       IND INN PCP COPAY                                  $10\n",
       " 6       FAM INN PCP COPAY                                  $10\n",
       " 7      IND INN SPEC COPAY                                  $25\n",
       " 8      FAM INN SPEC COPAY                                  $25\n",
       " 9   INN URGENT CARE COPAY                                  $10\n",
       " 10           INN ER COPAY                                 $230\n",
       " 11     IND INN DEDUCTIBLE                               $5,000\n",
       " 12     FAM INN DEDUCTIBLE                        $10,000 (Emb)\n",
       " 13        INN COINSURANCE                                  80%\n",
       " 14       IND INN OOP Max.                               $6,000\n",
       " 15       FAM INN OOP Max.                              $12,000\n",
       " 16         OON DEDUCTIBLE                                  N/A\n",
       " 17        OON COINSURANCE                                  N/A\n",
       " 18           OON OOP Max.                                  N/A,\n",
       "                                                               \n",
       " 0                    Name                 Greater Noida Mental\n",
       " 1         EFFECTIVE DATE                              1/1/2024\n",
       " 2               OPTION 2                              OPTION 2\n",
       " 3      PRIMARY/SOLD PLAN                     PRIMARY/SOLD PLAN\n",
       " 4           Benefit Plan   CPIJ Mod (Granite Advantage EPO OA)\n",
       " 5       IND INN PCP COPAY                                  $20\n",
       " 6       FAM INN PCP COPAY                                  $20\n",
       " 7      IND INN SPEC COPAY                                  $35\n",
       " 8      FAM INN SPEC COPAY                                  $35\n",
       " 9   INN URGENT CARE COPAY                                  $20\n",
       " 10           INN ER COPAY                                 $330\n",
       " 11     IND INN DEDUCTIBLE                               $6,000\n",
       " 12     FAM INN DEDUCTIBLE                        $12,000 (Emb)\n",
       " 13        INN COINSURANCE                                  90%\n",
       " 14       IND INN OOP Max.                               $7,000\n",
       " 15       FAM INN OOP Max.                              $12,000\n",
       " 16         OON DEDUCTIBLE                                  N/A\n",
       " 17        OON COINSURANCE                                  N/A\n",
       " 18           OON OOP Max.                                  N/A]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define the folder containing the Excel files\n",
    "folder_path = r\"C:\\Users\\shres\\Downloads\\Extracted_Tables_Tuftsss\"\n",
    "\n",
    "# Specify the group name you want to process\n",
    "group_name = \"Greater Noida Mental\"  # Change this to the desired group name\n",
    "\n",
    "# Call the function to process the specified group name in the folder\n",
    "split_dfs = process_excel_sheet(folder_path, group_name)\n",
    "split_dfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
